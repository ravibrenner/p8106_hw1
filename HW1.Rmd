---
title: "P8106 - HW1"
author: "Ravi Brenner"
output: 
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: 2
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

```{r}
library(glmnet)
library(caret)
library(tidymodels)
library(pls)
library(readr)
```

In this exercise, we predict the sale price of a house based on various characteristics. The
training data are in “housing train.csv”, and the test data are in “housing test.csv”. The
response is in the column “Sale price”, and other variables can be used as predictors. The
variable definitions can be found in “dictionary.txt”.

## Data and problem

Load data
```{r}
training <- read_csv("housing_training.csv")
testing <- read_csv("housing_test.csv")
```

Very brief exploration of the training data
```{r}
x <- model.matrix(Sale_Price ~ ., training)[,-1]
corrplot::corrplot(cor(x),
                   method = "circle", type = "full", tl.cex = 0.5)
```



## a. Lasso model
(a) Fit a lasso model on the training data. Report the selected tuning parameter and
the test error. When the 1SE rule is applied, how many predictors are included in
the model?

Fit model
```{r}
ctrl1 <- trainControl(method = "cv", number = 10)

set.seed(2025)
lasso.fit <- train(Sale_Price ~ .,
                   data = training,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(10, -5, length = 100))),
                   trControl = ctrl1)

plot(lasso.fit, xTrans = log)
```

Best tuning parameter
```{r}
best_lambda <- lasso.fit$bestTune$lambda
```

test error
```{r}
lasso.pred <- predict(lasso.fit, newdata = testing)
mean((lasso.pred - testing[, "Sale_Price"])^2 |> pull())
```


coefficients in the best lambda model
```{r}
coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda) |> as.matrix() |> as.data.frame() |> filter(s1 > 0) |> nrow()
```

coefficients in 1se model
First, find 1se lambda value
```{r}

rmse_1se <- lasso.fit$results |>
  filter(lambda == best_lambda) |>
  mutate(RMSE_plus_1se = RMSE + RMSESD) |>
  pull(RMSE_plus_1se)

lasso.fit$results |>
  filter(RMSE > 24500,
         RMSE < 26000)
# closest RMSE to 1SE is 25147.01, or when lambda = 2640.670

lambda_1se <- lasso.fit$results |>
  filter(RMSE > 24900,
         RMSE < 25500) |>
  pull(lambda)

ggplot(lasso.fit$results,aes(x = lambda, y = RMSE)) +
  geom_point() + 
  geom_errorbar(aes(ymin = RMSE - RMSESD, ymax = RMSE + RMSESD)) +
  scale_x_continuous(trans = "log") +
  geom_vline(xintercept = best_lambda) +
  geom_hline(yintercept = rmse_1se, color = "blue") +
  geom_vline(xintercept = lambda_1se)
```

model with 1se lambda value
```{r}
coef(lasso.fit$finalModel, lambda_1se) |> as.matrix() |> as.data.frame() |> filter(s1 > 0) |> nrow()
```


## b. Elastic net
(b) Fit an elastic net model on the training data. Report the selected tuning parameters
and the test error. Is it possible to apply the 1SE rule to select the tuning parameters
for elastic net? If the 1SE rule is applicable, implement it to select the tuning
parameters. If not, explain why.

Fit Elastic net model
```{r}
set.seed(2025)
enet.fit <- train(Sale_Price ~ .,
                  data = training,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(10, -5, length = 100))),
                  trControl = ctrl1)

enet.fit$bestTune

myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar, xTrans = log)

# coefficients in the final model
coef(enet.fit$finalModel, s = enet.fit$bestTune$lambda)

enet.fit$results |>
  filter(RMSE == min(RMSE)) |>
  mutate(RMSE_plus_1se = RMSE + RMSESD)


lasso.fit$results |>
  filter(RMSE > 25000,
         RMSE < 27000)
```


## c. Partial least squares
(c) Fit a partial least squares model on the training data and report the test error. How
many components are included in your model?

## d. Model comparison
(d) Choose the best model for predicting the response and explain your choice.

## e. Lasso using glmnet
(e) If R package “caret” was used for the lasso in (a), retrain this model using R package
“glmnet”, and vice versa. Compare the selected tuning parameters between the two
software approaches. Should there be discrepancies in the chosen parameters, discuss
potential reasons for these differences.