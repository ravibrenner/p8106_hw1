---
title: "P8106 - HW1"
author: "Ravi Brenner"
output: 
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: 2
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

```{r}
library(glmnet)
library(caret)
library(tidymodels)
library(pls)
library(readr)
```

In this exercise, we predict the sale price of a house based on various characteristics. The
training data are in “housing train.csv”, and the test data are in “housing test.csv”. The
response is in the column “Sale price”, and other variables can be used as predictors. The
variable definitions can be found in “dictionary.txt”.

## Data and problem

Load data
```{r}
training <- read_csv("housing_training.csv")
testing <- read_csv("housing_test.csv")
```

Very brief exploration of the training data
```{r}
x <- model.matrix(Sale_Price ~ ., training)[,-1]
corrplot::corrplot(cor(x),
                   method = "circle", type = "full", tl.cex = 0.5)
```



## a. Lasso model
(a) Fit a lasso model on the training data. Report the selected tuning parameter and
the test error. When the 1SE rule is applied, how many predictors are included in
the model?

Fit model
```{r}
ctrl1 <- trainControl(method = "cv", number = 10, selectionFunction = "best")

set.seed(2025)
lasso.fit <- train(Sale_Price ~ .,
                   data = training,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(10, -5, length = 100))),
                   trControl = ctrl1)

plot(lasso.fit, xTrans = log)
```

Best tuning parameter
```{r}
best_lambda <- lasso.fit$bestTune$lambda
best_lambda
```

test error
```{r}
lasso.pred <- predict(lasso.fit, newdata = testing)
mean((lasso.pred - testing[, "Sale_Price"])^2 |> pull()) #MSE
mean((lasso.pred - testing[, "Sale_Price"])^2 |> pull()) |> sqrt() #RMSE
```


How many ccoefficients in 1se model?
First, find 1se lambda value
```{r}
# fit 1se version
ctrl2 <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")
set.seed(2025)
lasso.1se <- train(Sale_Price ~ .,
                   data = training,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(10, -5, length = 100))),
                   trControl = ctrl2)

lambda_1se <- lasso.1se$bestTune |> pull(lambda)


rmse_1se <- lasso.fit$results |>
  filter(lambda == best_lambda) |>
  mutate(RMSESE = RMSESD / sqrt(12), # why sqrt(12)? in MSE formula is 1/sqrt(fold size)*SD. for RMSE do sqrt of that which is 12 for 1440/10
         RMSE_1SE = RMSE + RMSESE) |>
  pull(RMSE_1SE)

lasso.fit$results |>
  filter(RMSE > 23500,
         RMSE < 23600) 
```

model with 1se lambda value
```{r}
coef(lasso.fit$finalModel, lambda_1se) 

# Number of predictors
coef(lasso.fit$finalModel, lambda_1se) |> as.matrix() |> as.data.frame() |> filter(s1 > 0) |> nrow()
```


## b. Elastic net
(b) Fit an elastic net model on the training data. Report the selected tuning parameters
and the test error. Is it possible to apply the 1SE rule to select the tuning parameters
for elastic net? If the 1SE rule is applicable, implement it to select the tuning
parameters. If not, explain why.

Fit Elastic net model
```{r}
set.seed(2025)
enet.fit <- train(Sale_Price ~ .,
                  data = training,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(10, -5, length = 100))),
                  trControl = ctrl1)

enet.fit$bestTune

myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar, xTrans = log)

# coefficients in the final model
coef(enet.fit$finalModel, s = enet.fit$bestTune$lambda)
```

test error
```{r}
enet.pred <- predict(enet.fit, newdata = testing)
mean((enet.pred - testing[, "Sale_Price"])^2 |> pull()) #MSE
mean((enet.pred - testing[, "Sale_Price"])^2 |> pull()) |> sqrt() #RMSE
```

```{r}
# fit 1se version
ctrl2 <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")
set.seed(2025)
enet.1se <- train(Sale_Price ~ .,
                  data = training,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(10, -5, length = 100))),
                  trControl = ctrl2)

enet.1se$bestTune 

enet.fit$results |>
  filter(lambda == enet.fit$bestTune |> pull(lambda),
         alpha == enet.fit$bestTune |> pull(alpha)) |>
  mutate(RMSESE = RMSESD / sqrt(12), # why sqrt(12)? in MSE formula is 1/sqrt(fold size)*SD. for RMSE do sqrt of that which is 12 for 1440/10
         RMSE_1SE = RMSE + RMSESE) |>
  pull(RMSE_1SE)

enet.fit$results |>
  filter(RMSE > 23000,
         RMSE < 24600) 
```

It is very challenging to accurately apply the 1SE rule to the elastic net model, because there are multiple combinations of alpha and lambda that yield similar RMSE values which are close to the 1SE value. You could further refine the grid to create a more precise search for the exact 1SE value, but ultimately multiple combinations of alpha and lambda will remain as potential solutions, meaning it is not possible to select a single 1SE lambda value.


## c. Partial least squares
(c) Fit a partial least squares model on the training data and report the test error. How
many components are included in your model?

```{r}
set.seed(2025)

pls.fit <- train(Sale_Price ~ .,
                 data = training,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:(ncol(training)-1)),
                 trControl = ctrl1,
                 preProcess = c("center","scale"))

pred_pls <- predict(pls.fit, newdata = testing)
mean((testing$Sale_Price - pred_pls)^2) |> sqrt()

pls.fit$bestTune

plot(pls.fit)
```


## d. Model comparison
(d) Choose the best model for predicting the response and explain your choice.
```{r}
resamp <- resamples(list(lasso = lasso.fit, 
                         lasso_1se = lasso.1se, 
                         enet = enet.fit,
                         pls = pls.fit))
summary(resamp)

bwplot(resamp, metric = "RMSE")
```


## e. Lasso using glmnet
(e) If R package “caret” was used for the lasso in (a), retrain this model using R package
“glmnet”, and vice versa. Compare the selected tuning parameters between the two
software approaches. Should there be discrepancies in the chosen parameters, discuss
potential reasons for these differences.
```{r}
x <- model.matrix(Sale_Price ~ ., training)[,-1]
y <- training$Sale_Price

set.seed(2025)

lasso.glmnet <- cv.glmnet(x, y, 
                          alpha = 1, 
                          lambda = exp(seq(10, -5, length = 100)))

lasso.glmnet$lambda.min
lasso.glmnet$lambda.1se

plot(lasso.glmnet)
```

