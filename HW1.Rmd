---
title: "P8106 - HW1"
author: "Ravi Brenner"
output: 
  pdf_document:
    toc: yes
    toc_depth: 2
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

```{r}
library(glmnet)
library(caret)
library(tidymodels)
library(pls)
library(readr)
```


## Data and problem

Load data
```{r}
training <- read_csv("housing_training.csv")
testing <- read_csv("housing_test.csv")
```

Very brief exploration of the training data
```{r}
x <- model.matrix(Sale_Price ~ ., training)[,-1]
corrplot::corrplot(cor(x),
                   method = "circle", type = "full", tl.cex = 0.5)
```



## a. Lasso model
(a) Fit a lasso model on the training data. Report the selected tuning parameter and
the test error. When the 1SE rule is applied, how many predictors are included in
the model?

Fit model
```{r}
ctrl1 <- trainControl(method = "cv", number = 10, selectionFunction = "best")

set.seed(2025)
lasso.fit <- train(Sale_Price ~ .,
                   data = training,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(10, -5, length = 100))),
                   trControl = ctrl1)

plot(lasso.fit, xTrans = log)
```

Best tuning parameter
```{r}
best_lambda <- lasso.fit$bestTune$lambda
best_lambda
```

test error
```{r}
lasso.pred <- predict(lasso.fit, newdata = testing)
mean((lasso.pred - testing[, "Sale_Price"])^2 |> pull()) #MSE
mean((lasso.pred - testing[, "Sale_Price"])^2 |> pull()) |> sqrt() #RMSE
```


How many coefficients in 1se model?

First, find 1se lambda value
```{r}
# fit 1se version
ctrl2 <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")
set.seed(2025)
lasso.1se <- train(Sale_Price ~ .,
                   data = training,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(10, -5, length = 100))),
                   trControl = ctrl2)

lambda_1se <- lasso.1se$bestTune |> pull(lambda)
```

We can also calculate this value manually. The model output `results` gives the RMSE and RMSESD. To convert the SD to SE, we have to divide by the square root (because it is *root* mean square error) of the square root of the fold size (the standard way to calculate SE from SD). In this case the fold size is `r nrow(training)/10`, so we divide by the square root of 12.
```{r}

rmse_1se <- lasso.fit$results |>
  filter(lambda == best_lambda) |>
  mutate(RMSESE = RMSESD / sqrt(12), 
         RMSE_1SE = RMSE + RMSESE) |>
  pull(RMSE_1SE)

lasso.fit$results |>
  filter(RMSE > 23500,
         RMSE < 23600) 
```

Model with 1se lambda value:
```{r}
coef(lasso.fit$finalModel, lambda_1se) 

# Number of predictors
coef(lasso.fit$finalModel, lambda_1se) |> as.matrix() |> as.data.frame() |> filter(s1 > 0) |> nrow()
```

There are 18 predictors in the 1SE lasso model, compared to 25 predictors in the whole data set.


## b. Elastic net
(b) Fit an elastic net model on the training data. Report the selected tuning parameters
and the test error. Is it possible to apply the 1SE rule to select the tuning parameters
for elastic net? If the 1SE rule is applicable, implement it to select the tuning
parameters. If not, explain why.

Fit Elastic net model
```{r}
set.seed(2025)
enet.fit <- train(Sale_Price ~ .,
                  data = training,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(10, -5, length = 100))),
                  trControl = ctrl1)


myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar, xTrans = log)

# coefficients in the final model
coef(enet.fit$finalModel, s = enet.fit$bestTune$lambda)

```

Best tuning parameters alpha and lambda
```{r}
enet.fit$bestTune
```

test error
```{r}
enet.pred <- predict(enet.fit, newdata = testing)
mean((enet.pred - testing[, "Sale_Price"])^2 |> pull()) #MSE
mean((enet.pred - testing[, "Sale_Price"])^2 |> pull()) |> sqrt() #RMSE
```

Mechanically speaking, it is possible to apply the 1SE rule to elastic net:
```{r}
# fit 1se version
ctrl2 <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")
set.seed(2025)
enet.1se <- train(Sale_Price ~ .,
                  data = training,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(10, -5, length = 100))),
                  trControl = ctrl2)

enet.1se$bestTune 

enet.fit$results |>
  filter(lambda == enet.fit$bestTune |> pull(lambda),
         alpha == enet.fit$bestTune |> pull(alpha)) |>
  mutate(RMSESE = RMSESD / sqrt(12), 
         RMSE_1SE = RMSE + RMSESE) |>
  pull(RMSE_1SE)

enet.fit$results |>
  filter(RMSE > 23500,
         RMSE < 23600) 
```

Although it is possible to select a 1SE lambda value using the available caret functions, I do not think this is the best idea for elastic net. It is very challenging to accurately apply the 1SE rule to the elastic net model, because there are multiple combinations of alpha and lambda that yield similar RMSE values which are close to the 1SE value. You could further refine the grid to create a more precise search for the exact 1SE value, but ultimately multiple combinations of alpha and lambda will remain as potential solutions. The new value of lambda that corresponds to the "1SE solution" also comes with a different alpha value, meaning you are then jumping onto a different curve to compare RMSE values, and thereby changing the meaning of the lambda value too. It is therefore hard to interpret which combination of values is best with this added flexibility. Thus I would not use the 1SE method with elastic net.

## c. Partial least squares
(c) Fit a partial least squares model on the training data and report the test error. How
many components are included in your model?

```{r}
set.seed(2025)

pls.fit <- train(Sale_Price ~ .,
                 data = training,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:(ncol(training)-1)),
                 trControl = ctrl1,
                 preProcess = c("center","scale"))
```

PLS model test error
```{r}
pred_pls <- predict(pls.fit, newdata = testing)
mean((testing$Sale_Price - pred_pls)^2) |> sqrt() #RMSE
```

PLS number of components
```{r}
plot(pls.fit)

pls.fit$bestTune
```

There are 11 components in the optimal PLS model.

## d. Model comparison
(d) Choose the best model for predicting the response and explain your choice.
```{r}
resamp <- resamples(list(lasso = lasso.fit, 
                         lasso_1se = lasso.1se, 
                         enet = enet.fit,
                         pls = pls.fit))
summary(resamp)

bwplot(resamp, metric = "RMSE")
```

The models are fairly close, but I would opt to use the elastic net model to predict the Sale Price. It is a fairly flexible model and here yields the lowest average RMSE. The lasso and PLS models are both quite close in performance as well. The lasso 1SE model has worse prediction performance, but has fewer predictors and so could be preferred on that basis. However in this case, when I am just trying to optimize predictive power, I would choose the elastic net model.

That choice is made solely using training error. At this point we can compare the models based on test error. We can NOT use these results to inform model selection (since we specifically set aside the testing data for this purpose), but it can be interesting to see and understand how our chosen model did on out-of-sample data.

Testing error for each model
```{r}
# Lasso
mean((lasso.pred - testing$Sale_Price)^2) |> sqrt() 

# Lasso 1SE
mean((predict(lasso.1se,newdata = testing) - testing$Sale_Price)^2) |> sqrt() 

# Elastic Net
mean((enet.pred - testing$Sale_Price)^2) |> sqrt() 

# PLS
mean((pred_pls - testing$Sale_Price)^2) |> sqrt() 
```

It turned out that Elastic Net had the 2nd lowest test error, with the lowest belonging to the Lasso 1SE. Personally this is a good lesson for me for the future when it comes to model selection. The 1SE rule consciously selects a model that has worse training error than other "best fit" models. However by selecting a parsimonious set of predictors it may perform better on out of sample data. In all, I am satisfied with my choice of the elastic net model, since the testing error was fairly close to my expectations based on the training error.

## e. Lasso using glmnet
(e) If R package “caret” was used for the lasso in (a), retrain this model using R package
“glmnet”, and vice versa. Compare the selected tuning parameters between the two
software approaches. Should there be discrepancies in the chosen parameters, discuss
potential reasons for these differences.

Fitting lasso using glmnet
```{r}
x <- model.matrix(Sale_Price ~ ., training)[,-1]
y <- training$Sale_Price

set.seed(2025)

lasso.glmnet <- cv.glmnet(x, y, 
                          alpha = 1, 
                          lambda = exp(seq(10, -5, length = 100)))

plot(lasso.glmnet)

lasso.glmnet$lambda.min
lasso.glmnet$lambda.1se

# compare to the values from caret
lasso.fit$bestTune
lasso.1se$bestTune

```
The minimum lambda value using glmnet and caret was exactly the same. The 1SE lambda value was slightly different (~785 for glmnet and ~675 for caret), but a plausible explanation for this may caret choosing the first value below the 1SE cutoff and glmnet choosing the first value above above the 1SE cutoff, rather than a more substantial estimation difference between methods.
