---
title: "P8106 - HW1"
author: "Ravi Brenner"
output: 
  pdf_document:
    toc: yes
    toc_depth: 2
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

```{r}
library(glmnet)
library(caret)
library(tidymodels)
library(pls)
library(readr)
```


## Data and problem

Load data
```{r}
training <- read_csv("housing_training.csv")
testing <- read_csv("housing_test.csv")
```

Very brief exploration of the training data
```{r}
x <- model.matrix(Sale_Price ~ ., training)[,-1]
corrplot::corrplot(cor(x),
                   method = "circle", type = "full", tl.cex = 0.5)
```



## a. Lasso model
(a) Fit a lasso model on the training data. Report the selected tuning parameter and
the test error. When the 1SE rule is applied, how many predictors are included in
the model?

Fit model
```{r}
ctrl1 <- trainControl(method = "cv", number = 10, selectionFunction = "best")

set.seed(2025)
lasso.fit <- train(Sale_Price ~ .,
                   data = training,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(10, -5, length = 100))),
                   trControl = ctrl1)

plot(lasso.fit, xTrans = log)
```

Best tuning parameter
```{r}
best_lambda <- lasso.fit$bestTune$lambda
best_lambda
```

test error
```{r}
lasso.pred <- predict(lasso.fit, newdata = testing)
mean((lasso.pred - testing[, "Sale_Price"])^2 |> pull()) #MSE
mean((lasso.pred - testing[, "Sale_Price"])^2 |> pull()) |> sqrt() #RMSE
```


How many coefficients in 1se model?

First, find 1se lambda value
```{r}
# fit 1se version
ctrl2 <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")
set.seed(2025)
lasso.1se <- train(Sale_Price ~ .,
                   data = training,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(10, -5, length = 100))),
                   trControl = ctrl2)

lambda_1se <- lasso.1se$bestTune |> pull(lambda)
```

We can also calculate this value manually. The model output `results` gives the RMSE and RMSESD. To convert the SD to SE, we have to divide by the square root (because it is *root* mean square error) of the square root of the fold size (the standard way to calculate SE from SD). In this case the fold size is `r nrow(training)/10`, so we divide by the square root of 12.
```{r}

rmse_1se <- lasso.fit$results |>
  filter(lambda == best_lambda) |>
  mutate(RMSESE = RMSESD / sqrt(12), 
         RMSE_1SE = RMSE + RMSESE) |>
  pull(RMSE_1SE)

lasso.fit$results |>
  filter(RMSE > 23500,
         RMSE < 23600) 
```

Model with 1se lambda value:
```{r}
coef(lasso.fit$finalModel, lambda_1se) 

# Number of predictors
coef(lasso.fit$finalModel, lambda_1se) |> as.matrix() |> as.data.frame() |> filter(s1 > 0) |> nrow()
```

There are 18 predictors in the 1SE lasso model, compared to 25 predictors in the whole data set.


## b. Elastic net
(b) Fit an elastic net model on the training data. Report the selected tuning parameters
and the test error. Is it possible to apply the 1SE rule to select the tuning parameters
for elastic net? If the 1SE rule is applicable, implement it to select the tuning
parameters. If not, explain why.

Fit Elastic net model
```{r}
set.seed(2025)
enet.fit <- train(Sale_Price ~ .,
                  data = training,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(10, -5, length = 100))),
                  trControl = ctrl1)


myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar, xTrans = log)

# coefficients in the final model
coef(enet.fit$finalModel, s = enet.fit$bestTune$lambda)

```

Best tuning parameters alpha and lambda
```{r}
enet.fit$bestTune
```

test error
```{r}
enet.pred <- predict(enet.fit, newdata = testing)
mean((enet.pred - testing[, "Sale_Price"])^2 |> pull()) #MSE
mean((enet.pred - testing[, "Sale_Price"])^2 |> pull()) |> sqrt() #RMSE
```

Mechanically speaking, it is possible to apply the 1SE rule to elastic net:
```{r}
# fit 1se version
ctrl2 <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")
set.seed(2025)
enet.1se <- train(Sale_Price ~ .,
                  data = training,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(10, -5, length = 100))),
                  trControl = ctrl2)

enet.1se$bestTune 

enet.fit$results |>
  filter(lambda == enet.fit$bestTune |> pull(lambda),
         alpha == enet.fit$bestTune |> pull(alpha)) |>
  mutate(RMSESE = RMSESD / sqrt(12), 
         RMSE_1SE = RMSE + RMSESE) |>
  pull(RMSE_1SE)

enet.fit$results |>
  filter(RMSE > 23500,
         RMSE < 23600) 
```

Although it is possible to select a 1SE lambda value using the available caret functions, I do not think this is the best idea for elastic net. It is very challenging to accurately apply the 1SE rule to the elastic net model, because there are multiple combinations of alpha and lambda that yield similar RMSE values which are close to the 1SE value. You could further refine the grid to create a more precise search for the exact 1SE value, but ultimately multiple combinations of alpha and lambda will remain as potential solutions. The new value of lambda that corresponds to the "1SE solution" also comes with a different alpha value, meaning you are then jumping onto a different curve to compare RMSE values, and thereby changing the meaning of the lambda value too. It is therefore hard to interpret which combination of values is best with this added flexibility. Thus I would not use the 1SE method with elastic net.

## c. Partial least squares
(c) Fit a partial least squares model on the training data and report the test error. How
many components are included in your model?

```{r}
set.seed(2025)

pls.fit <- train(Sale_Price ~ .,
                 data = training,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:(ncol(training)-1)),
                 trControl = ctrl1,
                 preProcess = c("center","scale"))
```

PLS model test error
```{r}
pred_pls <- predict(pls.fit, newdata = testing)
mean((testing$Sale_Price - pred_pls)^2) |> sqrt() #RMSE
```

PLS number of components
```{r}
plot(pls.fit)

pls.fit$bestTune
```

There are 11 components in the optimal PLS model.

## d. Model selection

We can select the model based on the testing error. Since the models were trained on the training data only, choosing the model with lowest test error will give a less biased sense of which model will perform best on out-of-sample data, and is a good safeguard against overfit models

Testing error for each model
```{r}
# Lasso
mean((lasso.pred - testing$Sale_Price)^2) |> sqrt() 

# Lasso 1SE
mean((predict(lasso.1se,newdata = testing) - testing$Sale_Price)^2) |> sqrt() 

# Elastic Net
mean((enet.pred - testing$Sale_Price)^2) |> sqrt() 

# PLS
mean((pred_pls - testing$Sale_Price)^2) |> sqrt() 
```

It looks like the Lasso 1SE model has the lowest testing error, so it is the one I would select. This is interesting because, by definition, the 1SE model does not have the lowest possible training error. However because it uses fewer predictors, it is a simpler model, and this simplicity may serve it well when encountering out of sample data. It's possible that our models were somewhat overfit on the training data, and that a simpler model may perform better.


## e. Lasso using glmnet
(e) If R package “caret” was used for the lasso in (a), retrain this model using R package
“glmnet”, and vice versa. Compare the selected tuning parameters between the two
software approaches. Should there be discrepancies in the chosen parameters, discuss
potential reasons for these differences.

Fitting lasso using glmnet
```{r}
x <- model.matrix(Sale_Price ~ ., training)[,-1]
y <- training$Sale_Price

set.seed(2025)

lasso.glmnet <- cv.glmnet(x, y, 
                          alpha = 1, 
                          lambda = exp(seq(10, -5, length = 100)))

plot(lasso.glmnet)

lasso.glmnet$lambda.min
lasso.glmnet$lambda.1se

# compare to the values from caret
lasso.fit$bestTune
lasso.1se$bestTune

```
The minimum lambda value using glmnet and caret was exactly the same. The 1SE lambda value was slightly different (~785 for glmnet and ~675 for caret), but a plausible explanation for this may caret choosing the first value below the 1SE cutoff and glmnet choosing the first value above above the 1SE cutoff, rather than a more substantial estimation difference between methods.
